{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:Tensorflow needs to be built with TensorRT support enabled to allow TF-TRT to operate.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensorflow has not been built with TensorRT support.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39m#tgc = trt.TrtGraphConverterV2()\u001b[39;00m\n\u001b[0;32m      6\u001b[0m params \u001b[39m=\u001b[39m tensorflow\u001b[39m.\u001b[39m_api\u001b[39m.\u001b[39mv2\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mtensorrt\u001b[39m.\u001b[39mConversionParams(\n\u001b[0;32m      7\u001b[0m         precision_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFP16\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m         \u001b[39m# Set this to a large enough number so it can cache all the engines.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m         maximum_cached_engines\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m converter \u001b[39m=\u001b[39m tensorflow\u001b[39m.\u001b[39;49m_api\u001b[39m.\u001b[39;49mv2\u001b[39m.\u001b[39;49mexperimental\u001b[39m.\u001b[39;49mtensorrt\u001b[39m.\u001b[39;49mConverter(\n\u001b[0;32m     11\u001b[0m         input_saved_model_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msaved_model.pb\u001b[39;49m\u001b[39m\"\u001b[39;49m, conversion_params\u001b[39m=\u001b[39;49mparams)\n\u001b[0;32m     12\u001b[0m converter\u001b[39m.\u001b[39mconvert()\n\u001b[0;32m     13\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39minput_saved_model_dir = \"./model/saved_model.h5\" \u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39moutput_saved_model_dir = \"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mconverter = trt.TrtGraphConverter(input_saved_model_dir=input_saved_model_dir)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mconverter.convert()\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mconverter.save(output_saved_model_dir)'''\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bhavy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:561\u001b[0m, in \u001b[0;36mdeprecated_args.<locals>.deprecated_wrapper.<locals>.new_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m         _PRINTED_WARNING[(func, arg_name)] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    554\u001b[0m       logging\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    555\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mFrom \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: calling \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m (from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) with \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is deprecated and will \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    556\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mbe removed \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mInstructions for updating:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    559\u001b[0m           \u001b[39m'\u001b[39m\u001b[39min a future version\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m date \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mafter \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m date),\n\u001b[0;32m    560\u001b[0m           instructions)\n\u001b[1;32m--> 561\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bhavy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\compiler\\tensorrt\\trt_convert.py:1156\u001b[0m, in \u001b[0;36mTrtGraphConverterV2.__init__\u001b[1;34m(self, input_saved_model_dir, input_saved_model_tags, input_saved_model_signature_key, use_dynamic_shape, dynamic_shape_profile_strategy, max_workspace_size_bytes, precision_mode, minimum_segment_size, maximum_cached_engines, use_calibration, allow_build_at_runtime, conversion_params)\u001b[0m\n\u001b[0;32m   1147\u001b[0m \u001b[39mif\u001b[39;00m conversion_params \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1148\u001b[0m   conversion_params \u001b[39m=\u001b[39m TrtConversionParams(\n\u001b[0;32m   1149\u001b[0m       max_workspace_size_bytes\u001b[39m=\u001b[39mmax_workspace_size_bytes,\n\u001b[0;32m   1150\u001b[0m       precision_mode\u001b[39m=\u001b[39mprecision_mode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1153\u001b[0m       use_calibration\u001b[39m=\u001b[39muse_calibration,\n\u001b[0;32m   1154\u001b[0m       allow_build_at_runtime\u001b[39m=\u001b[39mallow_build_at_runtime)\n\u001b[1;32m-> 1156\u001b[0m _check_trt_version_compatibility()\n\u001b[0;32m   1157\u001b[0m _check_conversion_params(conversion_params, is_v2\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1159\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conversion_params \u001b[39m=\u001b[39m conversion_params\n",
      "File \u001b[1;32mc:\\Users\\bhavy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\compiler\\tensorrt\\trt_convert.py:221\u001b[0m, in \u001b[0;36m_check_trt_version_compatibility\u001b[1;34m()\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _pywrap_py_utils\u001b[39m.\u001b[39mis_tensorrt_enabled():\n\u001b[0;32m    217\u001b[0m   logging\u001b[39m.\u001b[39merror(\n\u001b[0;32m    218\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mTensorflow needs to be built with TensorRT support enabled to allow \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    219\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mTF-TRT to operate.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 221\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTensorflow has not been built with TensorRT support.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    223\u001b[0m \u001b[39mif\u001b[39;00m platform\u001b[39m.\u001b[39msystem() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWindows\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    224\u001b[0m   logging\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    225\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mWindows support is provided experimentally. No guarantee is made \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    226\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mregarding functionality or engineering support. Use at your own risk.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tensorflow has not been built with TensorRT support."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow._api.v2.experimental.tensorrt\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "import tensorflow.python.compiler.tensorrt.trt_convert as con\n",
    "#tgc = trt.TrtGraphConverterV2()\n",
    "params = tensorflow._api.v2.experimental.tensorrt.ConversionParams(\n",
    "        precision_mode='FP16',\n",
    "        # Set this to a large enough number so it can cache all the engines.\n",
    "        maximum_cached_engines=16)\n",
    "converter = tensorflow._api.v2.experimental.tensorrt.Converter(\n",
    "        input_saved_model_dir=\"saved_model.pb\", conversion_params=params)\n",
    "converter.convert()\n",
    "'''\n",
    "input_saved_model_dir = \"./model/saved_model.h5\" \n",
    "output_saved_model_dir = \"\"\n",
    "converter = trt.TrtGraphConverter(input_saved_model_dir=input_saved_model_dir)\n",
    "converter.convert()\n",
    "converter.save(output_saved_model_dir)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "            session, input_graph_def, output_names, freeze_var_names)\n",
    "        return frozen_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:Tensorflow needs to be built with TensorRT support enabled to allow TF-TRT to operate.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensorflow has not been built with TensorRT support.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m output_saved_model_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m params \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mtensorrt\u001b[39m.\u001b[39mConversionParams(\n\u001b[0;32m      8\u001b[0m     precision_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFP16\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m converter \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mexperimental\u001b[39m.\u001b[39;49mtensorrt\u001b[39m.\u001b[39;49mConverter(\n\u001b[0;32m     10\u001b[0m     input_saved_model_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m, conversion_params\u001b[39m=\u001b[39;49mparams)\n\u001b[0;32m     11\u001b[0m \u001b[39m#converter.convert()\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m#converter.save(output_saved_model_dir)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m converter \u001b[39m=\u001b[39m trt\u001b[39m.\u001b[39mTrtGraphConverter(input_saved_model_dir\u001b[39m=\u001b[39minput_saved_model_dir)\n",
      "File \u001b[1;32mc:\\Users\\bhavy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:561\u001b[0m, in \u001b[0;36mdeprecated_args.<locals>.deprecated_wrapper.<locals>.new_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m         _PRINTED_WARNING[(func, arg_name)] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    554\u001b[0m       logging\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    555\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mFrom \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: calling \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m (from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) with \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is deprecated and will \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    556\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mbe removed \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mInstructions for updating:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    559\u001b[0m           \u001b[39m'\u001b[39m\u001b[39min a future version\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m date \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mafter \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m date),\n\u001b[0;32m    560\u001b[0m           instructions)\n\u001b[1;32m--> 561\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bhavy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\compiler\\tensorrt\\trt_convert.py:1219\u001b[0m, in \u001b[0;36mTrtGraphConverterV2.__init__\u001b[1;34m(self, input_saved_model_dir, input_saved_model_tags, input_saved_model_signature_key, use_dynamic_shape, dynamic_shape_profile_strategy, max_workspace_size_bytes, precision_mode, minimum_segment_size, maximum_cached_engines, use_calibration, allow_build_at_runtime, conversion_params)\u001b[0m\n\u001b[0;32m   1210\u001b[0m \u001b[39mif\u001b[39;00m conversion_params \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1211\u001b[0m   conversion_params \u001b[39m=\u001b[39m TrtConversionParams(\n\u001b[0;32m   1212\u001b[0m       max_workspace_size_bytes\u001b[39m=\u001b[39mmax_workspace_size_bytes,\n\u001b[0;32m   1213\u001b[0m       precision_mode\u001b[39m=\u001b[39mprecision_mode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1216\u001b[0m       use_calibration\u001b[39m=\u001b[39muse_calibration,\n\u001b[0;32m   1217\u001b[0m       allow_build_at_runtime\u001b[39m=\u001b[39mallow_build_at_runtime)\n\u001b[1;32m-> 1219\u001b[0m _check_trt_version_compatibility()\n\u001b[0;32m   1220\u001b[0m _check_conversion_params(conversion_params, is_v2\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1222\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conversion_params \u001b[39m=\u001b[39m conversion_params\n",
      "File \u001b[1;32mc:\\Users\\bhavy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\compiler\\tensorrt\\trt_convert.py:224\u001b[0m, in \u001b[0;36m_check_trt_version_compatibility\u001b[1;34m()\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _pywrap_py_utils\u001b[39m.\u001b[39mis_tensorrt_enabled():\n\u001b[0;32m    220\u001b[0m   logging\u001b[39m.\u001b[39merror(\n\u001b[0;32m    221\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mTensorflow needs to be built with TensorRT support enabled to allow \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    222\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mTF-TRT to operate.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 224\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTensorflow has not been built with TensorRT support.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    226\u001b[0m \u001b[39mif\u001b[39;00m platform\u001b[39m.\u001b[39msystem() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWindows\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    227\u001b[0m   logging\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    228\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mWindows support is provided experimentally. No guarantee is made \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    229\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mregarding functionality or engineering support. Use at your own risk.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Tensorflow has not been built with TensorRT support."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "\n",
    "input_saved_model_dir = \"\" \n",
    "output_saved_model_dir = \"\"\n",
    "\n",
    "params = tf.experimental.tensorrt.ConversionParams(\n",
    "    precision_mode='FP16')\n",
    "converter = tf.experimental.tensorrt.Converter(\n",
    "    input_saved_model_dir=\"\", conversion_params=params)\n",
    "#converter.convert()\n",
    "#converter.save(output_saved_model_dir)\n",
    "\n",
    "\n",
    "converter = trt.TrtGraphConverter(input_saved_model_dir=input_saved_model_dir)\n",
    "converter.convert()\n",
    "converter.save(output_saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorrt\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtrt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# Load the HDF5 model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mload_model(\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m \u001b[39m# Convert the model to a TensorRT optimized model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m trt_model \u001b[39m=\u001b[39m trt\u001b[39m.\u001b[39mtensorrt\u001b[39m.\u001b[39mTRTModule()\n",
      "File \u001b[1;32mc:\\Users\\bhavy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\bhavy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\saving\\legacy\\save.py:227\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(filepath_str, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    226\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 227\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\n\u001b[0;32m    228\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo file or directory found at \u001b[39m\u001b[39m{\u001b[39;00mfilepath_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    229\u001b[0m         )\n\u001b[0;32m    231\u001b[0m     \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    232\u001b[0m         \u001b[39mreturn\u001b[39;00m saved_model_load\u001b[39m.\u001b[39mload(\n\u001b[0;32m    233\u001b[0m             filepath_str, \u001b[39mcompile\u001b[39m, options\n\u001b[0;32m    234\u001b[0m         )\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorrt as trt\n",
    "\n",
    "# Load the HDF5 model\n",
    "model = tf.keras.models.load_model('')\n",
    "\n",
    "# Convert the model to a TensorRT optimized model\n",
    "trt_model = trt.tensorrt.TRTModule()\n",
    "trt_model.from_keras(model)\n",
    "\n",
    "# Save the TensorRT model to an engine file\n",
    "trt_model.save('model.engine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Usage:\n",
    "    freeze_model.py --model=\"mymodel.h5\" --output=\"frozen_model.pb\"\n",
    "\n",
    "Note:\n",
    "    This requires that TensorRT is setup correctly. For more instructions, take a look at\n",
    "    https://docs.nvidia.com/deeplearning/sdk/tensorrt-install-guide/index.html\n",
    "'''\n",
    "import os\n",
    "\n",
    "from docopt import docopt\n",
    "import json\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "\n",
    "args = docopt(__doc__)\n",
    "in_model = os.path.expanduser(args['--model'])\n",
    "output = os.path.expanduser(args['--output'])\n",
    "output_path = Path(output)\n",
    "output_meta = Path('%s/%s.metadata' % (output_path.parent.as_posix(), output_path.stem))\n",
    "\n",
    "\n",
    "# Reset session\n",
    "tf.keras.backend.clear_session()\n",
    "tf.keras.backend.set_learning_phase(0)\n",
    "\n",
    "model = tf.keras.models.load_model(in_model, compile=False)\n",
    "session = tf.keras.backend.get_session()\n",
    "\n",
    "input_names = sorted([layer.op.name for layer in model.inputs])\n",
    "output_names = sorted([layer.op.name for layer in model.outputs])\n",
    "\n",
    "# Store additional information in metadata, useful when creating a TensorRT network\n",
    "meta = {'input_names': input_names, 'output_names': output_names}\n",
    "\n",
    "graph = session.graph\n",
    "\n",
    "# Freeze Graph\n",
    "with graph.as_default():\n",
    "    # Convert variables to constants\n",
    "    graph_frozen = tf.compat.v1.graph_util.convert_variables_to_constants(session, graph.as_graph_def(), output_names)\n",
    "    # Remove training nodes\n",
    "    graph_frozen = tf.compat.v1.graph_util.remove_training_nodes(graph_frozen)\n",
    "    with open(output, 'wb') as output_file, open(output_meta.as_posix(), 'w') as meta_file:\n",
    "        output_file.write(graph_frozen.SerializeToString())\n",
    "        meta_file.write(json.dumps(meta))\n",
    "\n",
    "    print ('Inputs = [%s], Outputs = [%s]' % (input_names, output_names))\n",
    "    print ('Writing metadata to %s' % output_meta.as_posix())\n",
    "    print ('To convert use: \\n   `convert-to-uff %s`' % (output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'builder' from 'google.protobuf.internal' (c:\\Users\\bhavy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\protobuf\\internal\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39monnx\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39monnx_tf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msaved_model\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bhavy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\onnx\\__init__.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39monnx_cpp2py_export\u001b[39;00m \u001b[39mimport\u001b[39;00m ONNX_ML  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39monnx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexternal_data_helper\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     load_external_data_for_model,\n\u001b[0;32m      9\u001b[0m     write_external_data_tensors,\n\u001b[0;32m     10\u001b[0m     convert_model_to_external_data,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39monnx_pb\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39monnx_operators_pb\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bhavy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\onnx\\external_data_helper.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mitertools\u001b[39;00m \u001b[39mimport\u001b[39;00m chain\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Callable, Iterable, Optional\n\u001b[1;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39monnx_pb\u001b[39;00m \u001b[39mimport\u001b[39;00m AttributeProto, GraphProto, ModelProto, TensorProto\n\u001b[0;32m     12\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mExternalDataInfo\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, tensor: TensorProto) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\bhavy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\onnx\\onnx_pb.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# This file is generated by setup.py. DO NOT EDIT!\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39monnx_ml_pb2\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bhavy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\onnx\\onnx_ml_pb2.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Generated by the protocol buffer compiler.  DO NOT EDIT!\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# source: onnx/onnx-ml.proto\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m\"\"\"Generated protocol buffer code.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m \u001b[39mimport\u001b[39;00m builder \u001b[39mas\u001b[39;00m _builder\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m descriptor \u001b[39mas\u001b[39;00m _descriptor\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m descriptor_pool \u001b[39mas\u001b[39;00m _descriptor_pool\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'builder' from 'google.protobuf.internal' (c:\\Users\\bhavy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\protobuf\\internal\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import onnx\n",
    "import onnx_tf\n",
    "\n",
    "model = tf.saved_model.load(\"\")\n",
    "\n",
    "onnx_model = onnx_tf.backend.prepare(model, target_opset=7)\n",
    "onnx.save(onnx_model, \"saved_model.onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ee691e3c2883c6a130fe7a4fe904a1eaf00aeea2af805a016ed0d0bd8e74479"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
