{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is TensorRT?\n",
    "\n",
    "TensorRT is an optimization tool provided by NVIDIA that applies graph optimization and layer fusion, and finds the fastest implementation of a deep learning model. In other words, TensorRT will optimize our deep learning model so that we expect a faster inference time than the original model (before optimization), such as 5x faster or 2x faster. The bigger model we have, the bigger space for TensorRT to optimize the model. Furthermore, this TensorRT supports all NVIDIA GPU devices, such as 1080Ti, Titan XP for Desktop, and Jetson TX1, TX2 for embedded device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard workflow for optimizing Tensorflow model to TensorRT\n",
    "\n",
    "![alt text](pictures/tf-trt_workflow.png)\n",
    "\n",
    "## Library I use in this video series\n",
    "Pre-requrement: Install TensorRT by following this tutorial [here](https://medium.com/@ardianumam/installing-tensorrt-in-ubuntu-dekstop-1c7307e1dcf6) for Ubuntu dekstop or [here](https://medium.com/@ardianumam/installing-tensorrt-in-jetson-tx2-8d130c4438f5) for Jetson devices\n",
    "\n",
    "1. Tensorflow 1.12\n",
    "2. OpenCV 3.4.5\n",
    "3. Pillow 5.2.0\n",
    "4. Numpy 1.15.2\n",
    "5. Matplotlib 3.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Read input: Tensorflow model and (b) Convert to frozen model (*.pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the needed libraries\n",
    "import tensorflow.compat.v1 as tf\n",
    "#import tensorflow.python.compiler.tensorrt as trt\n",
    "from tensorflow.python.platform import gfile\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Optimize the frozen model to TensorRT graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# has to be use this setting to make a session for TensorRT optimization\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39;49mcompat\u001b[39m.\u001b[39;49mv1\u001b[39m.\u001b[39;49mSession(config\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mConfigProto(gpu_options\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mGPUOptions(per_process_gpu_memory_fraction\u001b[39m=\u001b[39;49m\u001b[39m0.50\u001b[39;49m))) \u001b[39mas\u001b[39;00m sess:\n\u001b[0;32m      3\u001b[0m     \u001b[39m# import the meta graph of the tensorflow model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[39m#saver = tf.train.import_meta_graph(\"./model/tensorflow/big/model1.meta\")\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     saver \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mimport_meta_graph(\u001b[39m\"\u001b[39m\u001b[39m./Tensorflow-TensorRT/model/tensorflow/small/model_small.meta\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[39m# then, restore the weights to the meta graph\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[39m#saver.restore(sess, \"./model/tensorflow/big/model1\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bhavy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1604\u001b[0m, in \u001b[0;36mSession.__init__\u001b[1;34m(self, target, graph, config)\u001b[0m\n\u001b[0;32m   1583\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, target\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, graph\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, config\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1584\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a new TensorFlow session.\u001b[39;00m\n\u001b[0;32m   1585\u001b[0m \n\u001b[0;32m   1586\u001b[0m \u001b[39m  If no `graph` argument is specified when constructing the session,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1602\u001b[0m \u001b[39m        protocol buffer with configuration options for the session.\u001b[39;00m\n\u001b[0;32m   1603\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1604\u001b[0m   \u001b[39msuper\u001b[39;49m(Session, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(target, graph, config\u001b[39m=\u001b[39;49mconfig)\n\u001b[0;32m   1605\u001b[0m   \u001b[39m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m   1606\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_default_graph_context_manager \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bhavy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:712\u001b[0m, in \u001b[0;36mBaseSession.__init__\u001b[1;34m(self, target, graph, config)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    710\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    711\u001b[0m   \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39m_c_graph\u001b[39m.\u001b[39mget() \u001b[39mas\u001b[39;00m c_graph:\n\u001b[1;32m--> 712\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_session \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39;49mTF_NewSessionRef(c_graph, opts)\n\u001b[0;32m    713\u001b[0m   \u001b[39m# pylint: enable=protected-access\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m   tf_session\u001b[39m.\u001b[39mTF_DeleteSessionOptions(opts)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# has to be use this setting to make a session for TensorRT optimization\n",
    "with tf.compat.v1.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.50))) as sess:\n",
    "    # import the meta graph of the tensorflow model\n",
    "    #saver = tf.train.import_meta_graph(\"./model/tensorflow/big/model1.meta\")\n",
    "    saver = tf.train.import_meta_graph(\"./Tensorflow-TensorRT/model/tensorflow/small/model_small.meta\")\n",
    "    # then, restore the weights to the meta graph\n",
    "    #saver.restore(sess, \"./model/tensorflow/big/model1\")\n",
    "    saver.restore(sess, \"./Tensorflow-TensorRT/model/tensorflow/small/model_small\")\n",
    "    \n",
    "    # specify which tensor output you want to obtain \n",
    "    # (correspond to prediction result)\n",
    "    your_outputs = [\"output_tensor/Softmax\"]\n",
    "    \n",
    "    # convert to frozen model\n",
    "    frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "        sess, # session\n",
    "        tf.get_default_graph().as_graph_def(),# graph+weight from the session\n",
    "        output_node_names=your_outputs)\n",
    "    #write the TensorRT model to be used later for inference\n",
    "    with gfile.FastGFile(\"Tensorflow-TensorRT\\model\\saved_model.pb\",'wb') as f:\n",
    "        f.write(frozen_graph.SerializeToString())\n",
    "    print(\"Frozen model is successfully stored!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'converter' from 'tensorflow._api.v2.experimental.tensorrt' (c:\\Users\\bhavy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\_api\\v2\\experimental\\tensorrt\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m frozen_graph \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTensorflow-TensorRT\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\\\u001b[39m\u001b[39msaved_model.pb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensorrt\u001b[39;00m \u001b[39mimport\u001b[39;00m converter\n\u001b[0;32m      4\u001b[0m \u001b[39m# convert (optimize) frozen model to TensorRT model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m trt_graph \u001b[39m=\u001b[39m trt\u001b[39m.\u001b[39mcreate_inference_graph(\n\u001b[0;32m      6\u001b[0m     input_graph_def\u001b[39m=\u001b[39mfrozen_graph,\u001b[39m# frozen model\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     outputs\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39moutput_tensor/Softmax\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m      8\u001b[0m     max_batch_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\u001b[39m# specify your max batch size\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     max_workspace_size_bytes\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m(\u001b[39m10\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m9\u001b[39m),\u001b[39m# specify the max workspace\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     precision_mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFP32\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m# precision, can be \"FP32\" (32 floating point precision) or \"FP16\"\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'converter' from 'tensorflow._api.v2.experimental.tensorrt' (c:\\Users\\bhavy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\_api\\v2\\experimental\\tensorrt\\__init__.py)"
     ]
    }
   ],
   "source": [
    "frozen_graph = \"Tensorflow-TensorRT\\model\\saved_model.pb\"\n",
    "from tensorflow._api.v2.experimental.tensorrt import converter\n",
    "\n",
    "# convert (optimize) frozen model to TensorRT model\n",
    "trt_graph = trt.create_inference_graph(\n",
    "    input_graph_def=frozen_graph,# frozen model\n",
    "    outputs=[\"output_tensor/Softmax\"],\n",
    "    max_batch_size=2,# specify your max batch size\n",
    "    max_workspace_size_bytes=2*(10**9),# specify the max workspace\n",
    "    precision_mode=\"FP32\") # precision, can be \"FP32\" (32 floating point precision) or \"FP16\"\n",
    "\n",
    "#write the TensorRT model to be used later for inference\n",
    "with gfile.FastGFile(\"./TensorRT_model.engine\", 'wb') as f:\n",
    "    f.write(trt_graph.SerializeToString())\n",
    "print(\"TensorRT model is successfully stored!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Count how many nodes/operations before and after optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'node'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# check how many ops of the original frozen model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m all_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m([\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfrozen_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumb. of all_nodes in frozen graph:\u001b[39m\u001b[38;5;124m\"\u001b[39m, all_nodes)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# check how many ops that is converted to TensorRT engine\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'node'"
     ]
    }
   ],
   "source": [
    "# check how many ops of the original frozen model\n",
    "all_nodes = len([1 for n in frozen_graph.node])\n",
    "print(\"numb. of all_nodes in frozen graph:\", all_nodes)\n",
    "\n",
    "\n",
    "# check how many ops that is converted to TensorRT engine\n",
    "trt_engine_nodes = len([1 for n in trt_graph.node if str(n.op) == 'TRTEngineOp'])\n",
    "print(\"numb. of trt_engine_nodes in TensorRT graph:\", trt_engine_nodes)\n",
    "all_nodes = len([1 for n in trt_graph.node])\n",
    "print(\"numb. of all_nodes in TensorRT graph:\", all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pb_graph(model):\n",
    "  with gfile.FastGFile(model,'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "  return graph_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "NewRandomAccessFile failed to Create/Open: ./model/TensorRT-4_model.pb : The system cannot find the path specified.\r\n; No such process",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mSession(config\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mConfigProto(gpu_options\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mGPUOptions(per_process_gpu_memory_fraction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.50\u001b[39m))) \u001b[38;5;28;01mas\u001b[39;00m sess:\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;66;03m# read TensorRT model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m         trt_graph \u001b[38;5;241m=\u001b[39m \u001b[43mread_pb_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTENSORRT_MODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m#print()\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;66;03m# obtain the corresponding input-output tensor\u001b[39;00m\n\u001b[0;32m     11\u001b[0m         tf\u001b[38;5;241m.\u001b[39mimport_graph_def(trt_graph, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m, in \u001b[0;36mread_pb_graph\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gfile\u001b[38;5;241m.\u001b[39mFastGFile(model,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      3\u001b[0m   graph_def \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mGraphDef()\n\u001b[1;32m----> 4\u001b[0m   graph_def\u001b[38;5;241m.\u001b[39mParseFromString(\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_def\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:114\u001b[0m, in \u001b[0;36mFileIO.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    103\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the contents of a file as a string.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m  Starts reading from current position in file.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    string if in string (regular) mode.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preread_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    116\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:76\u001b[0m, in \u001b[0;36mFileIO._preread_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_check_passed:\n\u001b[0;32m     74\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mPermissionDeniedError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     75\u001b[0m                                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt open for reading\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_buf \u001b[38;5;241m=\u001b[39m \u001b[43m_pywrap_file_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBufferedInputStream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_to_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: NewRandomAccessFile failed to Create/Open: ./model/TensorRT-4_model.pb : The system cannot find the path specified.\r\n; No such process"
     ]
    }
   ],
   "source": [
    "# variable\n",
    "TENSORRT_MODEL_PATH = './model/TensorRT-4_model.pb'\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.50))) as sess:\n",
    "        # read TensorRT model\n",
    "        trt_graph = read_pb_graph(TENSORRT_MODEL_PATH)\n",
    "        #print()\n",
    "        # obtain the corresponding input-output tensor\n",
    "        tf.import_graph_def(trt_graph, name='')\n",
    "        input = sess.graph.get_tensor_by_name('input_tensor_input:0')\n",
    "        output = sess.graph.get_tensor_by_name('output_tensor/Softmax:0')\n",
    "        print(sess.graph.get_all_collection_keys())\n",
    "        # in this case, it demonstrates to perform inference for 50 times\n",
    "        total_time = 0; n_time_inference = 50\n",
    "        out_pred = sess.run(output, feed_dict={input: [[1,2,3,4,5,6,7,8,9,0]]})\n",
    "        for i in range(n_time_inference):\n",
    "            t1 = time.time()\n",
    "            out_pred = sess.run(output, feed_dict={input: [[1,2,3,4,5,6,7,8,9,0]]})\n",
    "            t2 = time.time()\n",
    "            delta_time = t2 - t1\n",
    "            total_time += delta_time\n",
    "            print(\"needed time in inference-\" + str(i) + \": \", delta_time)\n",
    "        avg_time_tensorRT = total_time / n_time_inference\n",
    "        print(\"average inference time: \", avg_time_tensorRT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ee691e3c2883c6a130fe7a4fe904a1eaf00aeea2af805a016ed0d0bd8e74479"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
